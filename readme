Term: Leader->master


Name: Vectory(A locality-satisfied sequential consistent protocol among auto-balanced shards)?
    enable locality for sequential consistency
    Consistency guarantee: local-read and Linearizable(shard-write).
    Automatically balanced(It based on the mastered shards, but can also be modified by CPU usage/Command frequencies etc.)


Three agreements:
Alive(appendEntries for all shards), with heartbeatTimeout and LeaderInterval
    * LeaderTimer: If leaderInterval, if any shard's log is not empty, do sendAppendEntry(shard), otherwise, send one sendAppendEntry to all other peers
    * ShardTimer[map<shard>Timeout]: If heartbeatTimeout of shard, run shard's election
    * Refresh: If appendEntry(m) <-  received, refresh shardTimer's timeout for all shards which master is m.
Shard(Balance-Raft), 3 phases(collect/elect, appendEntries, balance)
    [Collect]A pair <epoch, electNum([currentMastering value >> 16] + a randomNumber)> as an increasing number for vote(correct based on Paxos' voting number N)
                Collect also gathering the logs from all others, the highest log committed will be maintained(correct based on Paxos' highest choosing value)
    [AppendEntries] Raft append log (instead of raft need truncate and verify not majority committed log, we start from committed index.
    [Balance]
    * When Follower received AppendEntry, it compares [currentMastering shards] with electNum[winner's mastering shards], if current is smaller,
         Send a RPC(Balance) to call all others:
                1. majority response (can connect to the majority) &&
                2. acquire a token from the master(avoid concurrent election from multiple servers)
         If majority respond and token got, it start a newElection(Collect stage)
    * When Leader elected, it maintain a bool value of [Token].
         When received Balance:
                1. if it has Token, set token = false, and reply Token and a timer will regenerate Token after timeout.
                2. else, do not reply Token.
Vector(variant of Paxos, a total order of writes via virtual index)
   * A total order of writes among shards are voted by paxos algorithm. it uses a pair <Version, randomNumber> as proposal number n.
         When propose: (Version++, random<-)
         When accept: if(args.Version > version || (args.version == version && args.random >= v.random), accept it.
   * COC problem.


Here is the updates :
Intro: I add a contribution (provide define and proof) for CoPPar tree.
Background: Describe more about Composition order cycle.
Design: Talked the address problem(Composition) for the paper. Slightly modified CoPPar-cen/dec.
Impl: No change.
Perf: Add a discussion part in the end.

